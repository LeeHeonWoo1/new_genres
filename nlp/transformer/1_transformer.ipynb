{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "seq2seq의 단점을 보완하기 위해서 어텐션이 사용되었다. 하지만 어텐션을 RNN의 보정을 위한 용도로서 사용하는 것이 아니라 어텐션 만으로 인코더와 디코더를 만든다면?\n",
    "\n",
    "## Transformer의 주요 하이퍼 파라미터\n",
    "\n",
    "아래 수치들은 트랜스포머를 제안한 논문에서 사용한 수치로, 임의로 변경 가능한 값들이다.\n",
    "\n",
    "$$ d_{model} = 512 $$\n",
    "\n",
    "트랜스포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기를 의미한다. 임베딩 벡터의 차원 또한 $d_{model}$이며, 각 인코더와 디코더가 다음 층의 인코더와 디코더로 값을 보낼때에도 이 차원을 유지한다. 논문에서 제시하는 수치는 512 차원이다.\n",
    "\n",
    "$$ num\\_layers = 6 $$\n",
    "\n",
    "트랜스포머에서 하나의 인코더와 디코더를 층으로 생각했을 때, 트랜스포머 모델에서 인코더와 디코더가 몇 층으로 구성되는지를 의미한다. 논문에서는 인코더와 디코더를 각 6개씩 쌓았다.\n",
    "\n",
    "$$ num\\_heads = 8 $$\n",
    "\n",
    "여러개의 어텐션을 사용할 때, 한 번 하는 것 보다 여러 개로 분할해서 병렬로 어텐션을 수행하고 결과값을 다시 하나로 합치는 방식을 선택했다. 병렬의 개수를 의미한다.\n",
    "\n",
    "$$ d_{ff} = 2048 $$\n",
    "\n",
    "피드 포워드 신경망이 존재하기에, 해당 신경망의 은닉층의 크기를 의미한다.\n",
    "\n",
    "## Transformer\n",
    "\n",
    "트랜스포머는 RNN을 사용하지 않지만 기존의 seq2seq처럼 인코더에서 입력 시퀀스를 입력받고, 디코더에서 출력 시퀀스를 출력하는 인코더-디코더 구조를 유지하고 있다.\n",
    "\n",
    "<img src=\"./images_for_markdown/transformer.png\">\n",
    "\n",
    "위 그림은 인코더와 디코더가 6개씩 존재하는 트랜스포머의 구조를 보여준다.\n",
    "\n",
    "<img src=\"./images_for_markdown/transformer2.png\">\n",
    "\n",
    "디코더는 마치 기존의 seq2seq 구조 처럼 시작 심볼 sos(start of sentence) 를 입력으로 받아 종료 심볼 eos(end of sentence)가 나올 때까지 연산을 진행한다. 이는 RNN은 사용되지 않지마 여전히 인코더-디코더의 구조는 유지되고 있음을 보여준다.\n",
    "\n",
    "## 포지셔널 인코딩(Positional Encoding)\n",
    "\n",
    "트랜스포머의 입력에 대해서 알아보자. RNN이 자연어 처리에서 유용했던 이유는 단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의 특성으로 인해 각 단어의 위치 정보를 가질 수 있다는 점에 있었다.\n",
    "\n",
    "하지만 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줄 필요가 있다. 단어의 위치정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더해 모델의 입력으로 사용하는데, 이를 `Positional Encoding`이라고 한다.\n",
    "\n",
    "<img src=\"./images_for_markdown/positional_encoding.png\">\n",
    "\n",
    "위 그림은 입력으로 사용되는 임베딩 벡터들이 트랜스포머의 입력으로 사용되기 전에 포지셔널 인코딩의 값이 더해지는 것을 보여준다.\n",
    "\n",
    "<img src=\"./images_for_markdown/positional_encoding_cal.png\">\n",
    "\n",
    "트랜스포머는 위치 정보를 가진 값을 만들기 위해서 아래의 두 개의 함수를 사용한다.\n",
    "\n",
    "$$ PE(pos, 2i) = sin(pos/10000^{2i/d_{model}}) $$\n",
    "$$ PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}}) $$\n",
    "\n",
    "사인 함수와 코사인 함수의 그래프를 상기해보면 요동치는 값의 형태를 생각해볼 수 있는데, 트랜스포머는 사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해주므로서 단어의 순서 정보를 더하여 준다. 위 함수를 이해하기 위해서는 위에서 본 임베딩 벡터와 포지셔널 인코딩의 덧셈은 사실 임베딩 벡터가 모여 만들어진 문장 행렬과 포지셔널 인코딩 행렬의 덧셈 연산을 통해 이루어진다는 점을 이해해야 한다.\n",
    "\n",
    "<img src=\"./images_for_markdown/positional_encoding_functions.png\">\n",
    "\n",
    "$pos$는 입력 문장에서의 임베딩 벡터의 위치를 나타내며, $i$는 임베딩 벡터 내의 차원의 인덱스를 의미한다. 위의 식에 따르면 임베딩 벡터 내의 각 차원의 인덱스가 짝수인 경우에는 사인 함수의 값을 이용하고 홀수인 경우에는 코사인 함수의 값을 사용한다. 위의 수식에서 $(pos, 2i)$일때는 사인 함수, $(pos, 2i+1)$일 때는 코사인 함수를 사용하고 있음을 주목하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class PositionalEncoding():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
