{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 낙상 사고 감지 데이터셋\n",
    "\n",
    "해당 데이터 셋에서 다루는 주요 문제는 이벤트를 정상적인 생활 활동 또는 낙상 이벤트로 분류하는 문제이다. 목표는 가슴, 발목, 벨트와 같은 사람의 몸의 다양한 위치에서 수집한 센서 데이터를 기반으로 낙상 사례를 정확하게 식별할 수 있는 알고리즘을 개발하는 것이다.\n",
    "\n",
    "실시간으로 낙상을 감지하면 적절한 조치를 취하여 사람에게 적시에 도움을 제공하여 잠재적으로 생명을 구하고 낙상과 관련된 위험을 줄일 수 있다.\n",
    "\n",
    "### 필요 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.metrics import mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_csv_files(root_path):\n",
    "    filenames = os.listdir(root_path)\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    for filename in filenames:\n",
    "        csv_path = os.path.join(root_path, filename)\n",
    "        data_frame = pd.read_csv(csv_path)\n",
    "        data = pd.concat([data, data_frame]).reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = concat_csv_files(\"../data/fall_accident/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"./dataset.csv\", encoding=\"utf8\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.iloc[:, :-1]\n",
    "labels = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(labels)\n",
    "fig.update_layout(width=800, height=600)\n",
    "fig.update_yaxes(tickformat=\",\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=705)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(train_data[['x', 'y', 'z']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['x', 'y', 'z']] = scaler.transform(train_data[['x', 'y', 'z']])\n",
    "test_data[['x', 'y', 'z']] = scaler.transform(test_data[['x', 'y', 'z']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.values\n",
    "test_data = test_data.values\n",
    "\n",
    "train_labels = train_labels.astype(bool).values\n",
    "test_labels = test_labels.astype(bool).values\n",
    "\n",
    "n_train_data = train_data[~train_labels]\n",
    "n_test_data = test_data[~test_labels]\n",
    "\n",
    "an_train_data = train_data[train_labels]\n",
    "an_test_data = test_data[test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, Activation\n",
    "\n",
    "class detector(Model):\n",
    "    def __init__(self):\n",
    "        super(detector, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            Dense(128),\n",
    "            BatchNormalization(),\n",
    "            Activation(\"silu\"),\n",
    "            Dense(64),\n",
    "            BatchNormalization(),\n",
    "            Activation(\"silu\"),\n",
    "            Dense(32),\n",
    "            BatchNormalization(),\n",
    "            Activation(\"silu\"),\n",
    "            Dense(16),\n",
    "            BatchNormalization(),\n",
    "            Activation(\"silu\"),\n",
    "            Dense(8),\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            Dense(16),\n",
    "            BatchNormalization(),\n",
    "            Activation(\"silu\"),\n",
    "            Dense(32),\n",
    "            BatchNormalization(),\n",
    "            Activation(\"silu\"),\n",
    "            Dense(64),\n",
    "            BatchNormalization(),\n",
    "            Activation(\"silu\"),\n",
    "            Dense(128),\n",
    "            BatchNormalization(),\n",
    "            Activation(\"silu\"),\n",
    "            Dense(7, activation=\"sigmoid\"),\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        return decoded\n",
    "    \n",
    "autoencoder = detector()\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"mae\")\n",
    "autoencoder.fit(  # call \"call method\" in class \"detector\"\n",
    "    n_train_data, \n",
    "    n_train_data, \n",
    "    epochs=300, \n",
    "    validation_data=(n_test_data, n_test_data), \n",
    "    batch_size=512,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.3, mode=\"min\", patience=3),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed = autoencoder(n_train_data)  # 해당 시점에서도 call 메소드 호출\n",
    "train_loss = mae(reconstructed, n_train_data)  # auto encoder로 인해 reconstruct된 데이터와 훈련 데이터 간의 reconstruction error를 계산\n",
    "t = np.mean(train_loss) + np.std(train_loss)  # reconstruction error의 평균과 표준편차의 합으로 threshold를 설정\n",
    "\n",
    "def prediction(model, data, threshold):\n",
    "    rec = model(data)\n",
    "    loss = mae(rec, data)\n",
    "    return tf.math.less(loss, threshold)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = prediction(autoencoder, n_test_data, t)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
